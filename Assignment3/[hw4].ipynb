{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cordero, Palmsdale\n",
    "https://github.com/Pawmsdayl/CMSC197-ML/tree/main/Assignment3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "import re\n",
    "import html\n",
    "import string\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the list of files\n",
    "\n",
    "# train set: folders 0-70\n",
    "train_folders : list[str] = []\n",
    "for folder in range(71):\n",
    "    train_folders.append('data\\\\' + str(folder).rjust(3, '0'))\n",
    "\n",
    "train_files : list[str] = []\n",
    "for folder in train_folders:\n",
    "    for file in os.listdir(folder):\n",
    "        train_files.append(os.path.join(folder, file))\n",
    "\n",
    "# test set: folders 71-126\n",
    "test_folders : list[str] = []\n",
    "for folder in range(71, 127):\n",
    "    test_folders.append('data\\\\' + str(folder).rjust(3, '0'))\n",
    "\n",
    "test_files : list[str] = []\n",
    "for folder in test_folders:\n",
    "    for file in os.listdir(folder):\n",
    "        test_files.append(os.path.join(folder, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the list of stop words\n",
    "\n",
    "with open('stop_words.txt', 'r') as file:\n",
    "    stop_words = set()\n",
    "    for line in file:\n",
    "        if line.strip():\n",
    "            stop_words.add(line.strip().lower())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for preprocessing\n",
    "\n",
    "def decode_with_fallback(payload, encoding : str) -> str:\n",
    "    \"\"\"\n",
    "    Decode email using encoding. \n",
    "    If error occurs, fallback to utf-8 and latin-1.\n",
    "    \"\"\"\n",
    "    if encoding is None:\n",
    "        # default encoding\n",
    "        encoding = 'utf-8'                      \n",
    "    \n",
    "    try:\n",
    "        return payload.decode(encoding)\n",
    "    except (LookupError, UnicodeDecodeError):\n",
    "        try:\n",
    "            # fallback to default encoding\n",
    "            encoding = 'utf-8'                  \n",
    "            return payload.decode(encoding)\n",
    "        except UnicodeDecodeError:\n",
    "            # fallback to latin-1 encoding, ignore errors\n",
    "            encoding = 'latin-1'                \n",
    "            return payload.decode('latin-1')    \n",
    "\n",
    "def get_tokens_cleaned(email_path : str, exclude_stop_words : bool = True) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extracts the body of an email and returns a list of cleaned tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(email_path, 'rb') as file:\n",
    "        msg = BytesParser(policy=policy.default).parse(file)\n",
    "    \n",
    "    body = \"\"\n",
    "    \n",
    "    if msg.is_multipart():\n",
    "        for part in msg.iter_parts():\n",
    "            if part.get_content_type() in ['text/plain', 'text/html']:\n",
    "                payload = part.get_payload(decode=True)\n",
    "                encoding = part.get_content_charset()\n",
    "                body += decode_with_fallback(payload, encoding)\n",
    "    else:\n",
    "        payload = msg.get_payload(decode=True)\n",
    "        encoding = msg.get_content_charset()\n",
    "        body = decode_with_fallback(payload, encoding)   \n",
    "    \n",
    "    # remove HTML tags\n",
    "    body_no_html = re.sub(r'<.*?>', '', body)\n",
    "    \n",
    "    # decode HTML entities, removes \"&nbsp;\" and such\n",
    "    body_decoded = html.unescape(body_no_html)\n",
    "    \n",
    "    # convert to lowercase\n",
    "    body_lowercase = body_decoded.lower()\n",
    "    \n",
    "    # remove punctuation, numbers, and special characters\n",
    "    body_only_letters = body_lowercase.translate(str.maketrans('', '', string.punctuation + '0123456789~!@#$%^&*(){}[]\\\\/|<>;:'))\n",
    "    \n",
    "    # remove contractions\n",
    "    body_no_contraction = re.sub(r\"(\\w+)('ll|'ve|'re|'d|'m|'s|n't)\", r\"\\1\", body_only_letters)    \n",
    "    \n",
    "    # remove newlines and extra spaces\n",
    "    body_trimmed = re.sub(r'\\s+', ' ', body_no_contraction).strip()\n",
    "    \n",
    "    # tokenize body\n",
    "    tokens_dirty = body_trimmed.split()\n",
    "    \n",
    "    if not exclude_stop_words:\n",
    "        return tokens_dirty\n",
    "    \n",
    "    # remove stop words\n",
    "    tokens_cleaned : list[str] = []\n",
    "    for token in tokens_dirty:\n",
    "        if token not in stop_words:\n",
    "            tokens_cleaned.append(token)\n",
    "    \n",
    "    return tokens_cleaned\n",
    "\n",
    "def create_word_count(files : list[str], exclude_stop_words : bool = True) -> list[tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Returns a Counter of the top 10000 common words from a list of email files.\n",
    "    \"\"\"\n",
    "    \n",
    "    word_count = Counter()\n",
    "    \n",
    "    for file in files:\n",
    "        tokens_cleaned = get_tokens_cleaned(file, exclude_stop_words)\n",
    "        word_count.update(tokens_cleaned)\n",
    "\n",
    "    return word_count.most_common(10000)\n",
    "\n",
    "def create_dictionary(word_count : list[tuple[str, int]], min_count : int = 0) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a the words that appear at least min_count times.\n",
    "    \"\"\"\n",
    "    \n",
    "    dictionary : list[str] = []\n",
    "    for word, count in word_count:\n",
    "        if count >= min_count:\n",
    "            dictionary.append(word)\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "def get_token_lists(files : list[str], dictionary : list[str]) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Returns a list of clean and dictionary-filtered token lists, corresponding to each file.\n",
    "    \"\"\"\n",
    "    \n",
    "    token_lists : list[list[str]] = []\n",
    "    \n",
    "    for file in files:\n",
    "        tokens_cleaned = get_tokens_cleaned(file)\n",
    "        token_lists.append([token for token in tokens_cleaned if token in dictionary])\n",
    "    \n",
    "    return token_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the feature matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the labels\n",
    "\n",
    "labels : list[str] = []\n",
    "\n",
    "with open('labels', 'r') as file:\n",
    "    for line in file:\n",
    "        labels.append(line[:4].strip())\n",
    "\n",
    "train_labels = labels[:len(train_files)]\n",
    "test_labels = labels[len(train_files):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for creating dataframes\n",
    "\n",
    "def create_labeled_data(token_lists : list[list[str]], labels : list[str]) -> list[tuple[str, list[str]]]:\n",
    "    \"\"\"\n",
    "    Returns a list of tuples, each containing a label and a token list.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_zip = zip(labels, token_lists)\n",
    "    data_list = list(data_zip)\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "def create_feature_matrices(data : list[tuple[str, list[str]]], dictionary : list[str]) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns two dataframes, one for spam and one for ham, with columns corresponding to dictionary words.\n",
    "    \"\"\"\n",
    "    \n",
    "    row_skeleton = {word: 0 for word in dictionary}\n",
    "    spam_rows : list[dict] = []\n",
    "    ham_rows : list[dict] = []\n",
    "    \n",
    "    for datapoint in data:\n",
    "        row = row_skeleton.copy()\n",
    "    \n",
    "        for token in datapoint[1]:\n",
    "            if token in dictionary:\n",
    "                row[token] = 1\n",
    "        \n",
    "        if datapoint[0] == 'spam':\n",
    "            spam_rows.append(row)\n",
    "        else:\n",
    "            ham_rows.append(row)\n",
    "    \n",
    "    spam_df = pd.DataFrame(spam_rows)\n",
    "    ham_df = pd.DataFrame(ham_rows)\n",
    "    \n",
    "    return spam_df, ham_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the priors\n",
    "\n",
    "cat_count = Counter()\n",
    "\n",
    "cat_count.update(labels)\n",
    "\n",
    "for cat, count in cat_count.items():\n",
    "    if cat == 'spam':\n",
    "        spam_count = count\n",
    "    else:\n",
    "        ham_count = count\n",
    "\n",
    "prior_spam = spam_count / len(labels)\n",
    "prior_ham = ham_count / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing  the  Likelihood  of  each  word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for getting the probabilities\n",
    "\n",
    "def prob_given_cat(lam : float, dictionary_size : int, cat_word_count : int, word_in_cat_count : int) -> float:\n",
    "    \"\"\"\n",
    "    Returns the probability of a word given a category (spam/ham).\n",
    "    \"\"\"\n",
    "    \n",
    "    prob = (word_in_cat_count + lam) / (cat_word_count + (lam * dictionary_size))\n",
    "    \n",
    "    return prob\n",
    "\n",
    "def sum_occurences(df : pd.DataFrame) -> int:\n",
    "    \"\"\"\n",
    "    Returns the sum of all occurences in a dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    return df.sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying the emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for predicting the category\n",
    "\n",
    "\n",
    "def predict_category(tokens : list[str], lam : float, dictionary : list[str], spam_df : pd.DataFrame, ham_df : pd.DataFrame, spam_word_count : int, ham_word_count : int) -> str:\n",
    "    \"\"\"\n",
    "    Predicts the category (spam/ham) of an email.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    dictionary_size = len(dictionary)\n",
    "    \n",
    "    spam_prob = np.log(prior_spam)\n",
    "    ham_prob = np.log(prior_ham)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in dictionary:\n",
    "            word_in_spam_count = spam_df[token].sum()\n",
    "            spam_prob += np.log(prob_given_cat(\n",
    "                lam, dictionary_size, spam_word_count, word_in_spam_count\n",
    "            ))\n",
    "            \n",
    "            word_in_ham_count = ham_df[token].sum()\n",
    "            ham_prob += np.log(prob_given_cat(\n",
    "                lam, dictionary_size, ham_word_count, word_in_ham_count\n",
    "            ))\n",
    "    \n",
    "    if spam_prob > ham_prob:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'ham'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model\n",
    "\n",
    "cat_word_count = create_word_count(train_files)\n",
    "dictionary = create_dictionary(cat_word_count)\n",
    "\n",
    "train_token_lists = get_token_lists(train_files, dictionary)\n",
    "train_data = create_labeled_data(train_token_lists, train_labels)\n",
    "spam_df, ham_df = create_feature_matrices(train_data, dictionary)\n",
    "\n",
    "spam_word_count = sum_occurences(spam_df)\n",
    "ham_word_count = sum_occurences(ham_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the model\n",
    "\n",
    "test_token_lists = get_token_lists(test_files, dictionary)\n",
    "\n",
    "lam = 1\n",
    "predictions : list[str] = []\n",
    "\n",
    "for file in test_token_lists:\n",
    "    predictions.append(predict_category(file, lam, dictionary, spam_df, ham_df, spam_word_count, ham_word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for evaluating the model\n",
    "\n",
    "def accuracy(predictions : list[str], labels : list[str]) -> float:\n",
    "    \"\"\"\n",
    "    Returns the accuracy of the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    correct = 0\n",
    "    for pred, label in zip(predictions, labels):\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / len(labels)\n",
    "\n",
    "def recall(predictions : list[str], labels : list[str], category : str) -> float:\n",
    "    \"\"\"\n",
    "    Returns the recall of the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    true_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    for pred, label in zip(predictions, labels):\n",
    "        if pred == category and label == category:\n",
    "            true_positives += 1\n",
    "        if pred != category and label == category:\n",
    "            false_negatives += 1\n",
    "    \n",
    "    return true_positives / (true_positives + false_negatives)\n",
    "\n",
    "def precision(predictions : list[str], labels : list[str], category : str) -> float:\n",
    "    \"\"\"\n",
    "    Returns the precision of the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    \n",
    "    for pred, label in zip(predictions, labels):\n",
    "        if pred == category and label == category:\n",
    "            true_positives += 1\n",
    "        if pred == category and label != category:\n",
    "            false_positives += 1\n",
    "    \n",
    "    return true_positives / (true_positives + false_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9288221764919501\n",
      "recall spam:  0.9245621912887292\n",
      "recall ham:  0.9376276220530908\n",
      "precision spam:  0.9683943185024927\n",
      "precision ham:  0.8574096078764216\n"
     ]
    }
   ],
   "source": [
    "# evaluating the model\n",
    "\n",
    "orig_accuracy = accuracy(predictions, test_labels)\n",
    "orig_recall_spam = recall(predictions, test_labels, 'spam')\n",
    "orig_recall_ham = recall(predictions, test_labels, 'ham')\n",
    "orig_precision_spam = precision(predictions, test_labels, 'spam')\n",
    "orig_precision_ham = precision(predictions, test_labels, 'ham')\n",
    "\n",
    "\n",
    "print('accuracy: ', orig_accuracy)\n",
    "print('recall spam: ', orig_recall_spam)\n",
    "print('recall ham: ', orig_recall_ham)\n",
    "print('precision spam: ', orig_precision_spam)\n",
    "print('precision ham: ', orig_precision_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What  is  the  effect  of  removing  stop  words  in  terms  of  precision,  recall,  and accuracy?  Show a plot or a table of these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model with stop words (sw)\n",
    "\n",
    "sw_word_count = create_word_count(train_files, False)\n",
    "sw_dictionary = create_dictionary(sw_word_count)\n",
    "\n",
    "sw_train_token_lists = get_token_lists(train_files, sw_dictionary)\n",
    "sw_train_data = create_labeled_data(sw_train_token_lists, train_labels)\n",
    "sw_spam_df, sw_ham_df = create_feature_matrices(sw_train_data, sw_dictionary)\n",
    "\n",
    "sw_spam_word_count = sum_occurences(sw_spam_df)\n",
    "sw_ham_word_count = sum_occurences(sw_ham_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the model with stop words (sw)\n",
    "\n",
    "sw_test_token_lists = get_token_lists(test_files, sw_dictionary)\n",
    "\n",
    "lam = 1\n",
    "sw_predictions : list[str] = []\n",
    "\n",
    "for file in sw_test_token_lists:\n",
    "    sw_predictions.append(predict_category(file, lam, sw_dictionary, sw_spam_df, sw_ham_df, sw_spam_word_count, sw_ham_word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model with stop words (sw)\n",
    "\n",
    "sw_accuracy = accuracy(sw_predictions, test_labels)\n",
    "sw_recall_spam = recall(sw_predictions, test_labels, 'spam')\n",
    "sw_recall_ham = recall(sw_predictions, test_labels, 'ham')\n",
    "sw_precision_spam = precision(sw_predictions, test_labels, 'spam')\n",
    "sw_precision_ham = precision(sw_predictions, test_labels, 'ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>orig w/o sw</th>\n",
       "      <th>w/ stop words</th>\n",
       "      <th>difference (orig-sw)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.928822</td>\n",
       "      <td>0.929609</td>\n",
       "      <td>-0.000787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall spam</td>\n",
       "      <td>0.924562</td>\n",
       "      <td>0.925909</td>\n",
       "      <td>-0.001347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall ham</td>\n",
       "      <td>0.937628</td>\n",
       "      <td>0.937256</td>\n",
       "      <td>0.000371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>precision spam</td>\n",
       "      <td>0.968394</td>\n",
       "      <td>0.968257</td>\n",
       "      <td>0.000137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>precision ham</td>\n",
       "      <td>0.857410</td>\n",
       "      <td>0.859551</td>\n",
       "      <td>-0.002141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           metric  orig w/o sw  w/ stop words  difference (orig-sw)\n",
       "0        accuracy     0.928822       0.929609             -0.000787\n",
       "1     recall spam     0.924562       0.925909             -0.001347\n",
       "2      recall ham     0.937628       0.937256              0.000371\n",
       "3  precision spam     0.968394       0.968257              0.000137\n",
       "4   precision ham     0.857410       0.859551             -0.002141"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe for results\n",
    "\n",
    "results_sw = pd.DataFrame({\n",
    "    'metric': ['accuracy', 'recall spam', 'recall ham', 'precision spam', 'precision ham'],\n",
    "    'orig w/o sw': [orig_accuracy, orig_recall_spam, orig_recall_ham, orig_precision_spam, orig_precision_ham],\n",
    "    'w/ stop words': [sw_accuracy, sw_recall_spam, sw_recall_ham, sw_precision_spam, sw_precision_ham],\n",
    "    'difference (orig-sw)': [orig_accuracy - sw_accuracy, orig_recall_spam - sw_recall_spam, orig_recall_ham - sw_recall_ham, orig_precision_spam - sw_precision_spam, orig_precision_ham - sw_precision_ham]\n",
    "})\n",
    "\n",
    "results_sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, including stop words would lower all its metrics. This is because we are using useless words that have no relevant information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment  on  the  number  of  words  used  for  training.    Filter  the  dictionary  to include only words occurring more than k times (1000 words, then k > 100, and k = 50 times).  For example, the word “offer” appears 150 times, that means that it will be included in the dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the models with different min_count (mc) values\n",
    "\n",
    "min_counts = [50, 100, 1000]\n",
    "\n",
    "mc_dictionaries = []\n",
    "\n",
    "for count in min_counts:\n",
    "    dictionary = create_dictionary(cat_word_count, count)\n",
    "    mc_dictionaries.append(dictionary)\n",
    "\n",
    "mc_train_data = []\n",
    "mc_spam_df = []\n",
    "mc_ham_df = []\n",
    "mc_spam_word_count = []\n",
    "mc_ham_word_count = []\n",
    "\n",
    "for dictionary in mc_dictionaries:\n",
    "    train_token_lists = get_token_lists(train_files, dictionary)\n",
    "    train_data = create_labeled_data(train_token_lists, train_labels)\n",
    "    spam_df, ham_df = create_feature_matrices(train_data, dictionary)\n",
    "    spam_word_count = sum_occurences(spam_df)\n",
    "    ham_word_count = sum_occurences(ham_df)\n",
    "    \n",
    "    mc_train_data.append(train_data)\n",
    "    mc_spam_df.append(spam_df)\n",
    "    mc_ham_df.append(ham_df)\n",
    "    mc_spam_word_count.append(spam_word_count)\n",
    "    mc_ham_word_count.append(ham_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the models with different min_count values\n",
    "\n",
    "mc_predictions = []\n",
    "\n",
    "lam = 1\n",
    "\n",
    "for i in range(len(min_counts)):\n",
    "    test_token_lists = get_token_lists(test_files, mc_dictionaries[i])\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for file in test_token_lists:\n",
    "        predictions.append(predict_category(file, lam, mc_dictionaries[i], mc_spam_df[i], mc_ham_df[i], mc_spam_word_count[i], mc_ham_word_count[i]))\n",
    "    \n",
    "    mc_predictions.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the models with different min_count values\n",
    "\n",
    "mc_accuracies = []\n",
    "mc_recalls_spam = []\n",
    "mc_recalls_ham = []\n",
    "mc_precisions_spam = []\n",
    "mc_precisions_ham = []\n",
    "\n",
    "\n",
    "for i in range(len(min_counts)):\n",
    "    accuracy_value = accuracy(mc_predictions[i], test_labels)\n",
    "    recall_spam_value = recall(mc_predictions[i], test_labels, 'spam')\n",
    "    recall_ham_value = recall(mc_predictions[i], test_labels, 'ham')\n",
    "    precision_spam_value = precision(mc_predictions[i], test_labels, 'spam')\n",
    "    precision_ham_value = precision(mc_predictions[i], test_labels, 'ham')\n",
    "    \n",
    "    mc_accuracies.append(accuracy_value)\n",
    "    mc_recalls_spam.append(recall_spam_value)\n",
    "    mc_recalls_ham.append(recall_ham_value)\n",
    "    mc_precisions_spam.append(precision_spam_value)\n",
    "    mc_precisions_ham.append(precision_ham_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>min_count = 0</th>\n",
       "      <th>min_count = 50</th>\n",
       "      <th>min_count = 100</th>\n",
       "      <th>min_count = 1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.928822</td>\n",
       "      <td>0.928822</td>\n",
       "      <td>0.929427</td>\n",
       "      <td>0.886515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall spam</td>\n",
       "      <td>0.924562</td>\n",
       "      <td>0.925640</td>\n",
       "      <td>0.928514</td>\n",
       "      <td>0.898608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall ham</td>\n",
       "      <td>0.937628</td>\n",
       "      <td>0.935400</td>\n",
       "      <td>0.931316</td>\n",
       "      <td>0.861518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>precision spam</td>\n",
       "      <td>0.968394</td>\n",
       "      <td>0.967339</td>\n",
       "      <td>0.965450</td>\n",
       "      <td>0.930618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>precision ham</td>\n",
       "      <td>0.857410</td>\n",
       "      <td>0.858872</td>\n",
       "      <td>0.863066</td>\n",
       "      <td>0.804333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           metric  min_count = 0  min_count = 50  min_count = 100  \\\n",
       "0        accuracy       0.928822        0.928822         0.929427   \n",
       "1     recall spam       0.924562        0.925640         0.928514   \n",
       "2      recall ham       0.937628        0.935400         0.931316   \n",
       "3  precision spam       0.968394        0.967339         0.965450   \n",
       "4   precision ham       0.857410        0.858872         0.863066   \n",
       "\n",
       "   min_count = 1000  \n",
       "0          0.886515  \n",
       "1          0.898608  \n",
       "2          0.861518  \n",
       "3          0.930618  \n",
       "4          0.804333  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe for results\n",
    "\n",
    "results_mc = pd.DataFrame({\n",
    "    'metric': ['accuracy', 'recall spam', 'recall ham', 'precision spam', 'precision ham'],\n",
    "    'min_count = 0': [orig_accuracy, orig_recall_spam, orig_recall_ham, orig_precision_spam, orig_precision_ham],\n",
    "    'min_count = 50': [mc_accuracies[0], mc_recalls_spam[0], mc_recalls_ham[0], mc_precisions_spam[0], mc_precisions_ham[0]],\n",
    "    'min_count = 100': [mc_accuracies[1], mc_recalls_spam[1], mc_recalls_ham[1], mc_precisions_spam[1], mc_precisions_ham[1]],\n",
    "    'min_count = 1000': [mc_accuracies[2], mc_recalls_spam[2], mc_recalls_ham[2], mc_precisions_spam[2], mc_precisions_ham[2]],\n",
    "})\n",
    "\n",
    "results_mc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the min_count increases, all metrics decrease. This is because of the loss of information as we have less words to use as basis. The only benefit of this is faster processing time because of the decreased data needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss the results of the different parameters used for Lambda smoothing.  Test it  on  5  varying  values  of  the  λ  (e.g.  λ  =  2.0,  1.0,  0.5,  0.1,  0.005). Evaluate performance metrics for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the original model with different lambda (lam) values\n",
    "\n",
    "lam_predictions = []\n",
    "\n",
    "lams = [0.1, 0.5, 1, 2, 10]\n",
    "\n",
    "for lam in lams:\n",
    "    predictions = []\n",
    "    \n",
    "    for file in test_token_lists:\n",
    "        predictions.append(predict_category(file, lam, dictionary, spam_df, ham_df, spam_word_count, ham_word_count))\n",
    "    lam_predictions.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>lambda = 0.1</th>\n",
       "      <th>lambda = 0.5</th>\n",
       "      <th>lambda = 1</th>\n",
       "      <th>lambda = 2</th>\n",
       "      <th>lambda = 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.886152</td>\n",
       "      <td>0.886333</td>\n",
       "      <td>0.886515</td>\n",
       "      <td>0.886697</td>\n",
       "      <td>0.887544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall spam</td>\n",
       "      <td>0.898698</td>\n",
       "      <td>0.898698</td>\n",
       "      <td>0.898608</td>\n",
       "      <td>0.898788</td>\n",
       "      <td>0.899057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall ham</td>\n",
       "      <td>0.860219</td>\n",
       "      <td>0.860776</td>\n",
       "      <td>0.861518</td>\n",
       "      <td>0.861704</td>\n",
       "      <td>0.863746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>precision spam</td>\n",
       "      <td>0.930019</td>\n",
       "      <td>0.930278</td>\n",
       "      <td>0.930618</td>\n",
       "      <td>0.930717</td>\n",
       "      <td>0.931689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>precision ham</td>\n",
       "      <td>0.804235</td>\n",
       "      <td>0.804337</td>\n",
       "      <td>0.804333</td>\n",
       "      <td>0.804646</td>\n",
       "      <td>0.805435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           metric  lambda = 0.1  lambda = 0.5  lambda = 1  lambda = 2  \\\n",
       "0        accuracy      0.886152      0.886333    0.886515    0.886697   \n",
       "1     recall spam      0.898698      0.898698    0.898608    0.898788   \n",
       "2      recall ham      0.860219      0.860776    0.861518    0.861704   \n",
       "3  precision spam      0.930019      0.930278    0.930618    0.930717   \n",
       "4   precision ham      0.804235      0.804337    0.804333    0.804646   \n",
       "\n",
       "   lambda = 10  \n",
       "0     0.887544  \n",
       "1     0.899057  \n",
       "2     0.863746  \n",
       "3     0.931689  \n",
       "4     0.805435  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe for results\n",
    "\n",
    "results_lam = pd.DataFrame({\n",
    "    'metric': ['accuracy', 'recall spam', 'recall ham', 'precision spam', 'precision ham'],\n",
    "    'lambda = 0.1': [accuracy(lam_predictions[0], test_labels), recall(lam_predictions[0], test_labels, 'spam'), recall(lam_predictions[0], test_labels, 'ham'), precision(lam_predictions[0], test_labels, 'spam'), precision(lam_predictions[0], test_labels, 'ham')],\n",
    "    'lambda = 0.5': [accuracy(lam_predictions[1], test_labels), recall(lam_predictions[1], test_labels, 'spam'), recall(lam_predictions[1], test_labels, 'ham'), precision(lam_predictions[1], test_labels, 'spam'), precision(lam_predictions[1], test_labels, 'ham')],\n",
    "    'lambda = 1': [accuracy(lam_predictions[2], test_labels), recall(lam_predictions[2], test_labels, 'spam'), recall(lam_predictions[2], test_labels, 'ham'), precision(lam_predictions[2], test_labels, 'spam'), precision(lam_predictions[2], test_labels, 'ham')],    \n",
    "    'lambda = 2': [accuracy(lam_predictions[3], test_labels), recall(lam_predictions[3], test_labels, 'spam'), recall(lam_predictions[3], test_labels, 'ham'), precision(lam_predictions[3], test_labels, 'spam'), precision(lam_predictions[3], test_labels, 'ham')],\n",
    "    'lambda = 10': [accuracy(lam_predictions[4], test_labels), recall(lam_predictions[4], test_labels, 'spam'), recall(lam_predictions[4], test_labels, 'ham'), precision(lam_predictions[4], test_labels, 'spam'), precision(lam_predictions[4], test_labels, 'ham')],\n",
    "})\n",
    "\n",
    "results_lam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negligible difference could be because the test data has only a few number of words that the model was not trained on. Lambda really kicks in when there are a lot of this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are your recommendations to further improve the model?\n",
    "\n",
    "The instructions were quite limited in preprocessing, hence I took the initative to improve on it. This included filtering away HTML tags and enitities, numbers, and common English contractions. Proper preprocessing ensures that the tokens that the model will take into account are actually useful, which aligns with the trash-in trash-out principle. The instruction also used one hot encoding, wherein we only check for the presence of the word in the file. By doing this process, we lose some possibly valuable information. Hence, I also recommend using count of the word, instead of its presence. Reintroducing this data may give better insight to the model for predictions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
